{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0-preview is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Quick Tour of TensorFlow\n",
    "总体来说，TensorFlow可以支持跨平台，有以下几个优点：\n",
    "\n",
    "- 用法跟NumPy很像，但支持GPU\n",
    "- 支持分布式计算\n",
    "- 优化计算\n",
    "- 计算图可以跨平台\n",
    "\n",
    "它的python api 如下图：\n",
    "\n",
    "![](https://i.loli.net/2019/10/30/6bhsQuNj27f4V9W.png)\n",
    "\n",
    "我们在实际应用中，一般95%的任务会用到它的高层api，5%用到它的底层api，如下图所示：\n",
    "\n",
    "![](https://i.loli.net/2019/10/30/QeXaWDJE3AHjflT.png)\n",
    "\n",
    "可以在下边网站找到更多使用TensorFlow训练的模型和资源[https://github.com/jtoy/awesome-tensorflow](https://github.com/jtoy/awesome-tensorflow)\n",
    "\n",
    "## Using TensorFlow like Numpy\n",
    "这一小节讲解像Numpy一样使用TensorFlow。\n",
    "\n",
    "### Tensors and Operations\n",
    "在tf中，张量可以是一个多维数组，也可以是一个数值。可以使用`tf.constant()`来创建一个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65279, shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)>"
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # 一个多维数组，或者成为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65280, shape=(), dtype=int32, numpy=42>"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42) # 一个scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "像ndarray一个，张量也有shape和dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(2, 3)\n<dtype: 'float32'>\n"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "print(t.shape)\n",
    "print(t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65285, shape=(2, 2), dtype=float32, numpy=\narray([[2., 3.],\n       [5., 6.]], dtype=float32)>"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65289, shape=(2, 1), dtype=float32, numpy=\narray([[2.],\n       [5.]], dtype=float32)>"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "支持张量的操作符，下边仅仅展示了一部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65291, shape=(2, 3), dtype=float32, numpy=\narray([[11., 12., 13.],\n       [14., 15., 16.]], dtype=float32)>"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65292, shape=(2, 3), dtype=float32, numpy=\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)>"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65295, shape=(2, 2), dtype=float32, numpy=\narray([[14., 32.],\n       [32., 77.]], dtype=float32)>"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果想使用keras底层api，可以取出`keras.backend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65300, shape=(3, 2), dtype=float32, numpy=\narray([[11., 26.],\n       [14., 35.],\n       [19., 46.]], dtype=float32)>"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tensors and Numpy\n",
    "tf的张量和Numpy的数组可以无缝相互转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65301, shape=(3,), dtype=float64, numpy=array([2., 3., 4.])>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2., 3., 4.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65303, shape=(3,), dtype=float64, numpy=array([ 4.,  9., 16.])>"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)"
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "提示： Numpy默认的初始化类型为float64， 而TensorFlow默认的是float32，因为大多数神经网络用32位存储就足够了。可以通过设置`dtype=tf.float32`。\n",
    "\n",
    "### Type Conversions\n",
    "TensorFlow的类型转换是手动的，不允许不同类型的张量进行运算，这么做的好处有两个，一个是自动类型转换消耗系统性能，\n",
    "另一个是自动类型转换是隐式的，很容易忽略这些问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: add/\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40., dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65311, shape=(), dtype=float32, numpy=42.0>"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  0.],\n       [ 8., 10.,  1.]], dtype=float32)>"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'ResourceVariable' object does not support item assignment\n"
    }
   ],
   "source": [
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[100.,  42.,   0.],\n       [  8.,  10., 200.]], dtype=float32)>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[4., 5., 6.],\n       [1., 2., 3.]], dtype=float32)>"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3], [4., 5., 6.]], indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sparse tensors\n",
    "稀松张量表示张量数据中存在大量的0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SparseTensor(indices=tf.Tensor(\n[[0 1]\n [1 0]\n [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
    }
   ],
   "source": [
    "s = tf.SparseTensor(\n",
    "    indices=[[0, 1], [1, 0], [2, 3]],\n",
    "    values=[1., 2., 3.],\n",
    "    dense_shape=[3, 4]\n",
    ")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65347, shape=(3, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0.],\n       [2., 0., 0., 0.],\n       [0., 0., 0., 3.]], dtype=float32)>"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = s * 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
    }
   ],
   "source": [
    "# 稀松张量不能和其他类型的数据做+法运算，因为这种操作，稀松张量就被破坏了\n",
    "try:\n",
    "    s3 = s + 1.\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65351, shape=(3, 2), dtype=float32, numpy=\narray([[ 30.,  40.],\n       [ 20.,  40.],\n       [210., 240.]], dtype=float32)>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以做张量的乘法\n",
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "SparseTensor(indices=tf.Tensor(\n[[0 2]\n [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(\n",
    "    indices=[[0, 2], [0, 1]],\n",
    "    values=[1., 2.],\n",
    "    dense_shape=[3, 4]\n",
    ")\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "indices[1] = [0,1] is out of order [Op:SparseToDense]\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65360, shape=(3, 4), dtype=float32, numpy=\narray([[0., 2., 1., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tensor Arrays\n",
    "tf.TensorArray 是一个张量数组，内部保存的张量必须shape和dtype相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
    "array = array.write(0, tf.constant([1., 2.]))\n",
    "array = array.write(1, tf.constant([3., 10.]))\n",
    "array = array.write(2, tf.constant([5., 7.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65363, shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)>"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.read(1) # 取出后，以0填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65368, shape=(3, 2), dtype=float32, numpy=\narray([[1., 2.],\n       [0., 0.],\n       [5., 7.]], dtype=float32)>"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([2. 3.], shape=(2,), dtype=float32)\ntf.Tensor([4.6666665 8.666667 ], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "mean, variance = tf.nn.moments(array.stack(), axes=0)\n",
    "print(mean)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65378, shape=(), dtype=string, numpy=b'hello world'>"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(b\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65379, shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"café\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65380, shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([ord(c) for c in \"café\"])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65391, shape=(), dtype=int32, numpy=4>"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65395, shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233], dtype=int32)>"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### String arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65397, shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(p, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(p, \"UTF8\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>\n"
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ragged tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\n"
    }
   ],
   "source": [
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]>\n"
    }
   ],
   "source": [
    "print(r[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\n"
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[65, 66], [], [67]])\n",
    "print(tf.concat([r, r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n"
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n",
    "print(tf.concat([r, r3], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65547, shape=(4,), dtype=string, numpy=array([b'DEF', b'G', b'', b'HI'], dtype=object)>"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_encode(r3, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65612, shape=(4, 6), dtype=int32, numpy=\narray([[   67,    97,   102,   233,     0,     0],\n       [   67,   111,   102,   102,   101,   101],\n       [   99,    97,   102,   102,   232,     0],\n       [21654, 21857,     0,     0,     0,     0]], dtype=int32)>"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65619, shape=(2, 6), dtype=int32, numpy=\narray([[ 2,  3,  4,  5,  6,  7],\n       [ 0,  7,  9, 10,  0,  0]], dtype=int32)>"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\n",
    "set2 = tf.constant([[4, 5, 6], [9, 10, 0]])\n",
    "tf.sparse.to_dense(tf.sets.union(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65624, shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 7],\n       [7, 0, 0]], dtype=int32)>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65629, shape=(2, 2), dtype=int32, numpy=\narray([[ 4,  6],\n       [10,  0]], dtype=int32)>"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set2, set1)) # difference表示的是相对关系，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=65634, shape=(2, 2), dtype=int32, numpy=\narray([[5, 0],\n       [0, 9]], dtype=int32)>"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.intersection(set1, set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Customizing Models and Training Algorithms\n",
    "### Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"257.460937pt\" version=\"1.1\" viewBox=\"0 0 479.2525 257.460937\" width=\"479.2525pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 257.460937 \nL 479.2525 257.460937 \nL 479.2525 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 21.835 214.097812 \nL 468.235 214.097812 \nL 468.235 23.837812 \nL 21.835 23.837812 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 214.097812 \nL 21.835 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m707798c0d9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −4 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(12.989688 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 77.635 214.097812 \nL 77.635 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.635\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(68.789688 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 133.435 214.097812 \nL 133.435 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"133.435\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- −2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(124.589687 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 189.235 214.097812 \nL 189.235 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"189.235\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- −1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(180.389688 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 245.035 214.097812 \nL 245.035 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"245.035\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(241.2175 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 300.835 214.097812 \nL 300.835 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"300.835\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1 -->\n      <g transform=\"translate(297.0175 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 356.635 214.097812 \nL 356.635 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.635\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(352.8175 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 412.435 214.097812 \nL 412.435 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"412.435\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3 -->\n      <g transform=\"translate(408.6175 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 468.235 214.097812 \nL 468.235 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"468.235\" xlink:href=\"#m707798c0d9\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 4 -->\n      <g transform=\"translate(464.4175 230.215937)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- $z$ -->\n     <defs>\n      <path d=\"M 11.625 54.6875 \nL 54.296875 54.6875 \nL 52.6875 46.484375 \nL 11.53125 7.171875 \nL 45.515625 7.171875 \nL 44.09375 0 \nL -0.296875 0 \nL 1.3125 8.203125 \nL 42.484375 47.515625 \nL 10.203125 47.515625 \nz\n\" id=\"DejaVuSans-Oblique-122\"/>\n     </defs>\n     <g transform=\"translate(241.325 247.349375)scale(0.14 -0.14)\">\n      <use transform=\"translate(0 0.3125)\" xlink:href=\"#DejaVuSans-Oblique-122\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 214.097812 \nL 468.235 214.097812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5dff35ba23\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m5dff35ba23\" y=\"214.097812\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 218.656875)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 166.532812 \nL 468.235 166.532812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m5dff35ba23\" y=\"166.532812\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1 -->\n      <g transform=\"translate(7.2 171.091875)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 118.967812 \nL 468.235 118.967812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m5dff35ba23\" y=\"118.967812\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 123.526875)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 71.402812 \nL 468.235 71.402812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m5dff35ba23\" y=\"71.402812\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 75.961875)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 23.837812 \nL 468.235 23.837812 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"21.835\" xlink:href=\"#m5dff35ba23\" y=\"23.837812\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 28.396875)scale(0.12 -0.12)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_29\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 47.620312 \nL 192.319422 192.871867 \nL 196.805854 196.331052 \nL 201.292286 199.482754 \nL 205.778719 202.326973 \nL 210.265151 204.863709 \nL 214.751583 207.092962 \nL 219.238015 209.014732 \nL 223.724447 210.629018 \nL 228.210879 211.935822 \nL 232.697312 212.935142 \nL 237.183744 213.626979 \nL 241.670176 214.011333 \nL 246.156608 214.088204 \nL 250.64304 213.857591 \nL 255.129472 213.319496 \nL 259.615905 212.473917 \nL 264.102337 211.320855 \nL 268.588769 209.86031 \nL 273.075201 208.092282 \nL 277.561633 206.016771 \nL 282.048065 203.633777 \nL 286.534497 200.943299 \nL 291.02093 197.945339 \nL 295.507362 194.639895 \nL 299.993794 191.026968 \nL 360.560628 139.404031 \nL 468.235 47.620312 \nL 468.235 47.620312 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:2;\"/>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 77.226026 -1 \nL 82.401834 12.071778 \nL 89.131482 28.445255 \nL 95.861131 44.126896 \nL 102.590779 59.116699 \nL 109.320427 73.414665 \nL 116.050075 87.020794 \nL 122.779724 99.935086 \nL 129.509372 112.157541 \nL 133.995804 119.921491 \nL 138.482236 127.377957 \nL 142.968668 134.52694 \nL 147.455101 141.36844 \nL 151.941533 147.902457 \nL 156.427965 154.128991 \nL 160.914397 160.048041 \nL 165.400829 165.659608 \nL 169.887261 170.963693 \nL 174.373693 175.960294 \nL 178.860126 180.649412 \nL 183.346558 185.031047 \nL 187.83299 189.105198 \nL 192.319422 192.871867 \nL 196.805854 196.331052 \nL 201.292286 199.482754 \nL 205.778719 202.326973 \nL 210.265151 204.863709 \nL 214.751583 207.092962 \nL 219.238015 209.014732 \nL 223.724447 210.629018 \nL 228.210879 211.935822 \nL 232.697312 212.935142 \nL 237.183744 213.626979 \nL 241.670176 214.011333 \nL 246.156608 214.088204 \nL 250.64304 213.857591 \nL 255.129472 213.319496 \nL 259.615905 212.473917 \nL 264.102337 211.320855 \nL 268.588769 209.86031 \nL 273.075201 208.092282 \nL 277.561633 206.016771 \nL 282.048065 203.633777 \nL 286.534497 200.943299 \nL 291.02093 197.945339 \nL 295.507362 194.639895 \nL 299.993794 191.026968 \nL 304.480226 187.106558 \nL 308.966658 182.878664 \nL 313.45309 178.343288 \nL 317.939523 173.500429 \nL 322.425955 168.350086 \nL 326.912387 162.89226 \nL 331.398819 157.126951 \nL 335.885251 151.054159 \nL 340.371683 144.673884 \nL 344.858116 137.986125 \nL 349.344548 130.990884 \nL 353.83098 123.688159 \nL 358.317412 116.077951 \nL 362.803844 108.16026 \nL 367.290276 99.935086 \nL 374.019925 87.020794 \nL 380.749573 73.414665 \nL 387.479221 59.116699 \nL 394.208869 44.126896 \nL 400.938518 28.445255 \nL 407.668166 12.071778 \nL 412.843974 -1 \nL 412.843974 -1 \n\" style=\"fill:none;stroke:#0000ff;stroke-dasharray:1,1.65;stroke-dashoffset:0;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 189.235 214.097812 \nL 189.235 190.315312 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 300.835 214.097812 \nL 300.835 190.315312 \n\" style=\"fill:none;stroke:#ff0000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 21.835 214.097812 \nL 468.235 214.097812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_34\">\n    <path clip-path=\"url(#p22ddd1d4a5)\" d=\"M 245.035 214.097812 \nL 245.035 23.837812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 21.835 214.097812 \nL 21.835 23.837812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 468.235 214.097812 \nL 468.235 23.837812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 21.835 214.097812 \nL 468.235 214.097812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 21.835 23.837812 \nL 468.235 23.837812 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Huber loss -->\n    <defs>\n     <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 43.015625 \nL 55.515625 43.015625 \nL 55.515625 72.90625 \nL 65.375 72.90625 \nL 65.375 0 \nL 55.515625 0 \nL 55.515625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-72\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n    </defs>\n    <g transform=\"translate(207.960156 17.837812)scale(0.14 -0.14)\">\n     <use xlink:href=\"#DejaVuSans-72\"/>\n     <use x=\"75.195312\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"138.574219\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"202.050781\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"263.574219\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"304.6875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"336.474609\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"364.257812\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"425.439453\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"477.539062\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 31.635 207.097812 \nL 135.795 207.097812 \nQ 138.595 207.097812 138.595 204.297812 \nL 138.595 159.248437 \nQ 138.595 156.448437 135.795 156.448437 \nL 31.635 156.448437 \nQ 28.835 156.448437 28.835 159.248437 \nL 28.835 204.297812 \nQ 28.835 207.097812 31.635 207.097812 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 34.435 167.78625 \nL 62.435 167.78625 \n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:2;\"/>\n    </g>\n    <g id=\"line2d_36\"/>\n    <g id=\"text_17\">\n     <!-- huber($z$) -->\n     <defs>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g transform=\"translate(73.635 172.68625)scale(0.14 -0.14)\">\n      <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-104\"/>\n      <use transform=\"translate(63.378906 0.015625)\" xlink:href=\"#DejaVuSans-117\"/>\n      <use transform=\"translate(126.757812 0.015625)\" xlink:href=\"#DejaVuSans-98\"/>\n      <use transform=\"translate(190.234375 0.015625)\" xlink:href=\"#DejaVuSans-101\"/>\n      <use transform=\"translate(251.757812 0.015625)\" xlink:href=\"#DejaVuSans-114\"/>\n      <use transform=\"translate(292.871094 0.015625)\" xlink:href=\"#DejaVuSans-40\"/>\n      <use transform=\"translate(331.884766 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-122\"/>\n      <use transform=\"translate(384.375 0.015625)\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 34.435 190.997812 \nL 62.435 190.997812 \n\" style=\"fill:none;stroke:#0000ff;stroke-dasharray:1,1.65;stroke-dashoffset:0;\"/>\n    </g>\n    <g id=\"line2d_38\"/>\n    <g id=\"text_18\">\n     <!-- $\\frac{1}{2}z^2$ -->\n     <g transform=\"translate(73.635 195.897812)scale(0.14 -0.14)\">\n      <use transform=\"translate(0 43.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-49\"/>\n      <use transform=\"translate(0 -39.2375)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      <use transform=\"translate(57.036133 0.16875)\" xlink:href=\"#DejaVuSans-Oblique-122\"/>\n      <use transform=\"translate(113.992513 38.45)scale(0.7)\" xlink:href=\"#DejaVuSans-50\"/>\n      <path d=\"M 0 18.965625 \nL 0 25.215625 \nL 44.536133 25.215625 \nL 44.536133 18.965625 \nL 0 18.965625 \nz\n\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p22ddd1d4a5\">\n   <rect height=\"190.26\" width=\"446.4\" x=\"21.835\" y=\"23.837812\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<Figure size 576x252 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z**2 / 2, \"b:\", linewidth=1, label=r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1, -1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 107us/sample - loss: 0.6516 - mae: 1.0151 - val_loss: 0.2517 - val_mae: 0.5529\nEpoch 2/2\n11610/11610 [==============================] - 1s 49us/sample - loss: 0.2206 - mae: 0.5213 - val_loss: 0.1976 - val_mae: 0.4843\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x158130a58>"
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=huber_fn,\n",
    "    optimizer=\"nadam\", \n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and Loading Models That Contain Custom Components\n",
    "当我们自定义了一个loss函数后，我们保存模型，模型中会保存loss函数的name，因此在恢复模型的时候，还需要指定该name的真实函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 106us/sample - loss: 0.2072 - mae: 0.5003 - val_loss: 0.2018 - val_mae: 0.4856\nEpoch 2/2\n11610/11610 [==============================] - 1s 50us/sample - loss: 0.2019 - mae: 0.4928 - val_loss: 0.2014 - val_mae: 0.4855\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x112ca7cf8>"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"my_model_with_a_custom_loss.h5\",\n",
    "    custom_objects={\"huber_fn\": huber_fn}\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以上使用的huber_fn判断error时，认为-1 ~ 1之间的值为小的error，如果我们想自由控制这个threshhold的话，我们可以自定义一个创建函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshhold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshhold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshhold * tf.abs(error) - threshhold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 103us/sample - loss: 0.2237 - mae: 0.4908 - val_loss: 0.2360 - val_mae: 0.4853\nEpoch 2/2\n11610/11610 [==============================] - 1s 50us/sample - loss: 0.2185 - mae: 0.4858 - val_loss: 0.2053 - val_mae: 0.4667\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1117cef60>"
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "通过上边这种方式，虽然支持了再编译时传入threshhold，但是每次在恢复模型的时候，这个参数都要重新输入，下边演示下这个过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_threshhold.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 85us/sample - loss: 0.2161 - mae: 0.4817 - val_loss: 0.2280 - val_mae: 0.4768\nEpoch 2/2\n11610/11610 [==============================] - 1s 49us/sample - loss: 0.2131 - mae: 0.4774 - val_loss: 0.2284 - val_mae: 0.4792\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x10e2c6c88>"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"my_model_with_a_custom_loss_threshhold.h5\",\n",
    "    custom_objects={\"huber_fn\": create_huber(2.0)}\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果我们想保存这个threshold，就需要自定义继承自keras.losses.Loss的类。tf在保存模型时，会调用其内部的`get_config()`方法，因此我们需要在这个方法中，把threshold保存进去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshhold=1.0, **kwargs):\n",
    "        self.threshhold = threshhold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshhold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshhold * tf.abs(error) - self.threshhold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshhold\": self.threshhold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 110us/sample - loss: 0.8551 - mae: 0.9850 - val_loss: 0.5805 - val_mae: 0.6777\nEpoch 2/2\n11610/11610 [==============================] - 1s 53us/sample - loss: 0.2559 - mae: 0.5211 - val_loss: 0.4196 - val_mae: 0.5879\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x113454470>"
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "# model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other Custom Functions\n",
    "在大多数时间内，我们都使用类似上边代码中的函数或类别，包括losses, regularizers, constraints, initializers, metrics,activation functions, layers, full models。这么多神经网络的核心内容，我们都可以custom。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=86589, shape=(), dtype=int32, numpy=6>"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 102us/sample - loss: 1.5798 - val_loss: inf\nEpoch 2/2\n11610/11610 [==============================] - 1s 48us/sample - loss: 0.5827 - val_loss: inf\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x11aacd6d8>"
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        activation=my_softplus,\n",
    "        kernel_regularizer=my_l1_regularizer,\n",
    "        kernel_initializer=my_glorot_initializer,\n",
    "        kernel_constraint=my_positive_weights\n",
    "    )\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metric=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nmodel = keras.models.load_model(\\n    \"my_model_with_many_custom_parts.h5\",\\n    custom_objects={\\n       \"my_l1_regularizer\": my_l1_regularizer(0.01),\\n       \"my_positive_weights\": my_positive_weights,\\n       \"my_glorot_initializer\": my_glorot_initializer,\\n       \"my_softplus\": my_softplus,\\n    })\\n'"
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: \n",
    "\"\"\"\n",
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer(0.01),\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def call(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 128us/sample - loss: 1.5869 - val_loss: 1.9949\nEpoch 2/2\n11610/11610 [==============================] - 1s 50us/sample - loss: 0.6274 - val_loss: 1.2327\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x154523080>"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(\n",
    "        1,\n",
    "        activation=my_softplus,\n",
    "        kernel_regularizer=MyL1Regularizer(0.01),\n",
    "        kernel_initializer=my_glorot_initializer,\n",
    "        kernel_constraint=my_positive_weights\n",
    "    )\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metric=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.load_model(\n",
    "#     \"my_model_with_many_custom_parts.h5\",\n",
    "#     custom_objects={\n",
    "#        \"my_l1_regularizer\": MyL1Regularizer,\n",
    "#        \"my_positive_weights\": my_positive_weights,\n",
    "#        \"my_glorot_initializer\": my_glorot_initializer,\n",
    "#        \"my_softplus\": my_softplus,\n",
    "#     })"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Metrics\n",
    "Warning: if you use the same function as the loss and a metric, you may be surprised to see different results. This is generally just due to floating point precision errors: even though the mathematical equations are equivalent, the operations are not run in the same order, which can lead to small differences. Moreover, when using sample weights, there's more than just precision errors:\n",
    "\n",
    "- the loss since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses).\n",
    "- the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing.\n",
    "\n",
    "举个例子，我们有一个二元分类器，第一个batch中，true positive 有4个，预测为positive有5个，也就是说，有一个预测错误了，那么这时候的精度为4/5为80%，第二batch，true positive 有0个，错误预测3个，那么这一batch的精度为0，前两个的精度为40%。\n",
    "\n",
    "但是metric，不是这样算的， 它是全部正确的数除以总数，也就是4/8 为50%。\n",
    "\n",
    "它会记录整个过程中的某写值，这种计算方式成为streaming metrics。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 90us/sample - loss: 1.8537 - huber_fn: 0.7876\nEpoch 2/2\n11610/11610 [==============================] - 0s 40us/sample - loss: 0.5842 - huber_fn: 0.2583\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x113a81b00>"
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.)])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.), optimizer=\"nadam\", metrics=[create_huber(2.)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 101us/sample - loss: 0.1168 - huber_fn: 0.2367\nEpoch 2/2\n11610/11610 [==============================] - 1s 46us/sample - loss: 0.1123 - huber_fn: 0.2270\n"
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0.11678683142740083, 0.11744123564995271)"
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_fn\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99256, shape=(), dtype=float32, numpy=0.8>"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99303, shape=(), dtype=float32, numpy=0.5>"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99312, shape=(), dtype=float32, numpy=0.5>"
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99359, shape=(), dtype=float32, numpy=14.0>"
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = HuberMetric(2.)\n",
    "\n",
    "# total = 2 * |10 - 2| - 2²/2 = 14\n",
    "# count = 1\n",
    "# result = 14 / 1 = 14\n",
    "m(tf.constant([[2.]]), tf.constant([[10.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99390, shape=(), dtype=float32, numpy=7.0>"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total = total + (|1 - 0|² / 2) + (2 * |9.25 - 5| - 2² / 2) = 14 + 7 = 21\n",
    "# count = count + 2 = 3\n",
    "# result = total / count = 21 / 3 = 7\n",
    "m(tf.constant([[0.], [5.]]), tf.constant([[1.], [9.25]]))\n",
    "\n",
    "m.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=21.0>,\n <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>]"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>,\n <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>]"
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reset_states()\n",
    "m.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Layers\n",
    "有一些layer是没有weights的，可以使用下边这种方式来处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=99547, shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对于回归任务，如果需要预测的值是positive并且存在不同的scales，比如0.001， 10， 100000 ，在最后添加一个exponential layer 很有必要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "=================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 24us/sample - loss: nan\n"
    },
    {
     "data": {
      "text/plain": "nan"
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer,\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid_scaled, y_valid)\n",
    ")\n",
    "\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation=keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        print(\"==\", batch_input_shape.as_list()[:-1])\n",
    "        self.kernel = self.add_weight(name=\"kernel\", shape=[batch_input_shape[-1], self.units])\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        print(\"==_+_+_+__+_+_\")\n",
    "        # print(tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]))\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "== [None]\n== [None]\nTrain on 11610 samples, validate on 3870 samples\nEpoch 1/5\n11610/11610 [==============================] - 1s 98us/sample - loss: 1.7101 - val_loss: 2.3730\nEpoch 2/5\n11610/11610 [==============================] - 1s 44us/sample - loss: 0.6065 - val_loss: 0.6987\nEpoch 3/5\n11610/11610 [==============================] - 1s 45us/sample - loss: 0.4848 - val_loss: 0.4787\nEpoch 4/5\n11610/11610 [==============================] - 1s 44us/sample - loss: 0.4299 - val_loss: 0.4049\nEpoch 5/5\n11610/11610 [==============================] - 1s 45us/sample - loss: 0.4027 - val_loss: 0.4736\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x10ecab630>"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    validation_data=(X_valid_scaled, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_layer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "== [None]\n== [None]\n"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\", custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "对于有多个输入层的处理方式是，传入的数据格式为字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        print(X)\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        print(\"===\")\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(<tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'input_4:0' shape=(None, 2) dtype=float32>)\n"
    }
   ],
   "source": [
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(<tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'input_4:0' shape=(None, 2) dtype=float32>)\n(<tf.Tensor 'my_multi_layer_3/Identity:0' shape=(None, 2) dtype=float32>, <tf.Tensor 'my_multi_layer_3/Identity_1:0' shape=(None, 2) dtype=float32>)\n"
    }
   ],
   "source": [
    "print(MyMultiLayer()((inputs1, inputs2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 90us/sample - loss: 0.3912 - val_loss: 0.3615\nEpoch 2/2\n11610/11610 [==============================] - 1s 46us/sample - loss: 0.3779 - val_loss: 0.5536\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x113e0dba8>"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    validation_data=(X_valid_scaled, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Model\n",
    "自定义模型这个功能很强大，基于此我们可以定制各种各样的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"n_layers\": self.n_layers, \"n_neurons\": self.n_neurons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"output_dim\": self.output_dim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/5\n11610/11610 [==============================] - 2s 181us/sample - loss: 5.3022\nEpoch 2/5\n11610/11610 [==============================] - 1s 57us/sample - loss: 1.4829\nEpoch 3/5\n11610/11610 [==============================] - 1s 56us/sample - loss: 0.6444\nEpoch 4/5\n11610/11610 [==============================] - 1s 58us/sample - loss: 0.8442\nEpoch 5/5\n11610/11610 [==============================] - 1s 58us/sample - loss: 2.0899\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x12afb0588>"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 47us/sample - loss: 0.4956\n"
    },
    {
     "data": {
      "text/plain": "0.7059432318044263"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/5\n11610/11610 [==============================] - 2s 141us/sample - loss: 0.6676\nEpoch 2/5\n11610/11610 [==============================] - 1s 57us/sample - loss: 0.6705\nEpoch 3/5\n11610/11610 [==============================] - 1s 58us/sample - loss: 0.4807\nEpoch 4/5\n11610/11610 [==============================] - 1s 55us/sample - loss: 0.9774\nEpoch 5/5\n11610/11610 [==============================] - 1s 54us/sample - loss: 0.4443\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x11423a6d8>"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block1 = ResidualBlock(2, 30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1,\n",
    "    block1,\n",
    "    block1,\n",
    "    block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Losses and Metrics Based on Model Internals\n",
    "如果我们想在模型内部操作loss或者metrics的话，需要使用`add_loss()`方法，接下来，我们使用下边的代码演示这样一个案例。\n",
    "\n",
    "我们先建立一个5个影隐藏层的网络，然后再添加一个reconstruction的隐藏层，该层于输入层的神经元个数相同。然后计算该层与输入层的平方差。然后把该平方差乘以一个基数后加到主模型的loss上，。这个基数不能很大，不然会影响主模型的loss。\n",
    "\n",
    "这么做的目的是，能够让网络尽可能的保留输入信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\") for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/2\n11610/11610 [==============================] - 2s 163us/sample - loss: 0.8105\nEpoch 2/2\n11610/11610 [==============================] - 1s 61us/sample - loss: 0.4459\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1144535c0>"
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing Gradients Using Autodiff\n",
    "TensorFlow使用反向传播的算法来计算变量的梯度。\n",
    "\n",
    "我们先定义一个简单的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "学过微积分的应该都明白。f对w1的偏导数为$6w1 + 2w2$,对w2的偏导数为$2w1$。假设w1=5，w2=3 ,那么他们的偏导数为36，10 。\n",
    "\n",
    "那么在计算机中应该如何计算函数的梯度呢？ \n",
    "\n",
    "一种方法是，相对于每个参数，计算他们每一小步的变化率，这样做的缺点是计算量很大。计算的值只是一个近似值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "36.000003007075065"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f((w1 + eps), w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10.000000003174137"
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, (w2 + eps)) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf中使用`tf.GradientTape()`来把函数放入上下文，然后调用`gradient()`函数计算地图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=130853, shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: id=130845, shape=(), dtype=float32, numpy=10.0>]"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 注意： 调用gradient后，就会释放上下文，如果再次调用就会抛出异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "GradientTape.gradient can only be called once on non-persistent tapes.\n"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z= f(w1, w2)\n",
    "dz_w1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_w2 = tape.gradient(z, w2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "异常给出的解释是，如果调用多次，需要设置persistent为True， 但是在最终调用完成后，需要del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "dz_w1 = tape.gradient(z, w1)\n",
    "dz_w2 = tape.gradient(z, w2)\n",
    "del ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: id=130901, shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: id=130906, shape=(), dtype=float32, numpy=10.0>)"
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_w1, dz_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果给的是tf.constant. 默认返回的值为None， 需要在上下文中通过watch来查看其值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[None, None]"
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=130965, shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: id=130957, shape=(), dtype=float32, numpy=10.0>]"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果要对一组函数求导怎么办呢？ 默认的 他们把这一组函数相加后，再求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131041, shape=(), dtype=float32, numpy=136.0>,\n <tf.Tensor: id=131042, shape=(), dtype=float32, numpy=30.0>]"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我们验证一下计算的结果，f对w1的偏导数为$6w1 + 2w2$,对w2的偏导数为$2w1$。假设w1=5，w2=3，那么z1=f(5, 5), z1对\bw1的偏导数为6*5+2*5，值为40，z2=f(5, 8), z2对\bw1的偏导数为6*5+2*8，值为46，z3=f(5, 10), z3对\bw1的偏导数为6*5+2*10，值为50， 这三个值相加 40+46+50等于136。同样，我们用代码也可以验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tf.Tensor([136.  30.], shape=(2,), dtype=float32)\n"
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "s = tf.reduce_sum(tf.stack([tape.gradient(z, [w1, w2]) for z in (z1, z2, z3)]), axis=0)\n",
    "print(s)\n",
    "del tape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果想获取一组函数中的单独的偏导数，只需要调用`jabobian()`函数就可以了，下边在演示一下如何计算二阶偏导数的方法，由此可以引申出更多阶的求导方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "hessians =  [hessian_tape.gradient(jacobian, [w1, w2]) for jacobian in jacobians]\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131150, shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: id=131142, shape=(), dtype=float32, numpy=10.0>]"
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[<tf.Tensor: id=131159, shape=(), dtype=float32, numpy=6.0>,\n  <tf.Tensor: id=131161, shape=(), dtype=float32, numpy=2.0>],\n [<tf.Tensor: id=131166, shape=(), dtype=float32, numpy=2.0>, None]]"
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "还有更多的控制，我们知道tf使用的是反向传播求导，如果我们想在反向求导的时候，忽略loss中的某些表达式，也可以实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131186, shape=(), dtype=float32, numpy=30.0>, None]"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z= f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "解释一下，对于上边代码中的函数f，由于在求导的时候，禁止了w2的计算，因此所有对于w2的求导就都变成了None。\b\n",
    "\n",
    "有时候，对于某些函数，可能会返回Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=131191, shape=(), dtype=float32, numpy=30.0>"
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(tf.exp(tf.constant(30., dtype=tf.float32)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=131196, shape=(), dtype=float32, numpy=inf>"
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.log(tf.exp(tf.constant(100., dtype=tf.float32)) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "这是因为数值超越了最大值造成的，在做除法的时候，造成了无限大除以无限大，因此tf会返回Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131212, shape=(), dtype=float32, numpy=nan>]"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131232, shape=(), dtype=float32, numpy=1.0>]"
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(100.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "上边代码中，通过@tf.custom_gradient 可以给函数额外附加一个单独计算梯度的方法，函数正常求值的时候走返回的第一个值，求梯度的时候，走给的自定义梯度函数。\n",
    "\n",
    "我们也可以通过tf.where，做一些判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_better_softplus(z):\n",
    "    return tf.where(z > 30, z, tf.math.log(tf.exp(z) + 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: id=131356, shape=(), dtype=float32, numpy=nan>]"
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(1000.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Training Loops\n",
    "自定义循环有很多步骤，我们首先定义一个模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "写一个随机生成batch的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "写一个打印进度的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \"_\".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{}\".format(iteration, total) + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "50/50loss: 0.0900_mean_square: 858.5000\n"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "上边代码中的进度是没有进度条的，下边的代码可以实现这个功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(iteration, total, size=30):\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' 3500/10000 [=>....]'"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress_bar(3500, 10000, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \"_\".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "50/50 [==============================] - loss: 0.0900_mean_square: 858.5000\n"
    }
   ],
   "source": [
    "mean_loss = keras.metrics.Mean(name=\"loss\")\n",
    "mean_square = keras.metrics.Mean(name=\"mean_square\")\n",
    "for i in range(1, 50 + 1):\n",
    "    loss = 1 / i\n",
    "    mean_loss(loss)\n",
    "    mean_square(i ** 2)\n",
    "    print_status_bar(i, 50, mean_loss, [mean_square])\n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "定义好在循环中用到的外部变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/5\n11610/11610 [==============================] - mean: 0.6304_mean_absolute_error: 0.5141\nEpoch 2/5\n11610/11610 [==============================] - mean: 0.6153_mean_absolute_error: 0.5068\nEpoch 3/5\n11610/11610 [==============================] - mean: 0.6252_mean_absolute_error: 0.5089\nEpoch 4/5\n11610/11610 [==============================] - mean: 0.6467_mean_absolute_error: 0.5145\nEpoch 5/5\n11610/11610 [==============================] - mean: 0.6408_mean_absolute_error: 0.5192\n"
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+ 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        # 随机取出batch数据\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 计算预测值\n",
    "            y_pred = model(X_batch)\n",
    "            # 计算主要loss\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            # 把主要loss和次要loss相加。如果有正则化，可能会有次loss\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        # 计算梯度\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        # 根据梯度计算训练变量\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        #  添加约束\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        # 求loss的均值\n",
    "        mean_loss(loss)\n",
    "        # 计算metrcs\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        # 打印进度\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Final result:  4.0\n"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Mean()\n",
    "m([1, 3, 5, 7])\n",
    "print('Final result: ', m.result().numpy())  # Final result: 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Final result:  6.888889\n"
    }
   ],
   "source": [
    "m([1, 3, 5, 7, 30])\n",
    "print('Final result: ', m.result().numpy())  # Final result: 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024717, shape=(), dtype=float32, numpy=8.0>"
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.eager.def_function.Function at 0x1c5471ac8>"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024723, shape=(), dtype=int32, numpy=8>"
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024731, shape=(), dtype=float32, numpy=8.0>"
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF Functions and Concrete Functions\n",
    "tf的函数本质上是多态的，针对不同的输入组合，它会自动生成相对应的concrete function，这些输入组合被称为输入信号。\n",
    "\n",
    "因此，对于tf的函数而言，相同的输入信号，tf会复用以前的concrete function。 。比如调用`tf_cube(tf.constant(2.))`后再调用`tf_cube(tf.constant(3.))`，使用的就是同一个concrete function。但是如果调用`tf_cube(tf.constant([2.]))`,就会生成一个新的concrete function。\n",
    "\n",
    "我们再以`tf_cube(2)`和`tf_cube(tf.constant(2.))`为例，说明一下这其中的区别。\n",
    "\n",
    "![](https://i.loli.net/2019/11/10/zZOICxHVQ8nua65.png)\n",
    "\n",
    "上图中的根函数为tf_cube, 当输入设置2的时候，生成了左边的结构，它包含两部分，一个是FunctionDef，表示函数定义，用来定义输入输出，另一是FuncGraph，表示函数图，用于说明函数计算过程。\n",
    "\n",
    "对于左边的concrete function而言，图的机构生成了一个固定的数值8，函数定义部分，没有输入，说明使用该concrete function，不管输入什么，都是返回结果8.\n",
    "\n",
    "对于右边的图的结构，接受一个类型为floate32的输入，然后计算它的平方在输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = tf_cube.get_concrete_function(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.framework.func_graph.FuncGraph at 0x12ce7acc0>"
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024734, shape=(), dtype=float32, numpy=8.0>"
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function is tf_cube.get_concrete_function(tf.constant(3.))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring Function Definitions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Operation 'x' type=Placeholder>,\n <tf.Operation 'pow/y' type=Const>,\n <tf.Operation 'pow' type=Pow>,\n <tf.Operation 'Identity' type=Identity>]"
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = concrete_function.graph.get_operations()\n",
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(y):\n",
    "    return y ** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Operation 'y' type=Placeholder>,\n <tf.Operation 'pow/y' type=Const>,\n <tf.Operation 'pow' type=Pow>,\n <tf.Operation 'Identity' type=Identity>]"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_a = tf.function(a)\n",
    "tf_a_conrect = tf_a.get_concrete_function(tf.constant(5.))\n",
    "ops1 = tf_a_conrect.graph.get_operations()\n",
    "ops1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'x:0' shape=() dtype=float32>,\n <tf.Tensor 'pow/y:0' shape=() dtype=float32>]"
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op = ops[2]\n",
    "list(pow_op.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor 'pow:0' shape=() dtype=float32>]"
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Operation 'x' type=Placeholder>"
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_operation_by_name(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor 'Identity:0' shape=() dtype=float32>"
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_tensor_by_name(\"Identity:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "name: \"__inference_cube_1024730\"\ninput_arg {\n  name: \"x\"\n  type: DT_FLOAT\n}\noutput_arg {\n  name: \"identity\"\n  type: DT_FLOAT\n}"
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.function_def.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How TF Functions Trace Python Functions to Extract Their Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(\"print:\", x)\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "print: Tensor(\"x:0\", shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "result = tf_cube(tf.constant(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024754, shape=(), dtype=float32, numpy=8.0>"
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "从上边的打印，可以看出来，result的结果没什么问题。print函数是python函数，并不能编译到tf的graph中，因此在生成concrete function的时候只会调用一次，除非参数的类型变了，生成另一个新的concrete function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "W1112 10:44:05.637765 140734905206208 def_function.py:474] 5 out of the last 5 calls to <function tf_cube at 0x12675f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\nW1112 10:44:05.652945 140734905206208 def_function.py:474] 6 out of the last 6 calls to <function tf_cube at 0x12675f8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\nprint: 2\nprint: 3\nprint: Tensor(\"x:0\", shape=(1, 2), dtype=float32)\nprint: Tensor(\"x:0\", shape=(2, 2), dtype=float32)\nprint: Tensor(\"x:0\", shape=(3, 2), dtype=float32)\n"
    }
   ],
   "source": [
    "result = tf_cube(2)\n",
    "result = tf_cube(3)\n",
    "result = tf_cube(tf.constant([[1. , 2.]]))\n",
    "result = tf_cube(tf.constant([[1., 2.], [3., 4.]]))\n",
    "result = tf_cube(tf.constant([[1., 2.], [3., 4.], [5., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tf_cube(tf.constant([[7., 7.], [7., 7.], [7., 7.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果我们想为函数自定一个固定的参数类型，也就是固定参数签名。如果参数类型不匹配，就会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
    "def shrink(images):\n",
    "    print(\"Tracing\", images)\n",
    "    return images[:, ::2, ::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tracing Tensor(\"images:0\", shape=(None, 28, 28), dtype=float32)\n"
    }
   ],
   "source": [
    "img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\n",
    "img_batch_2 = tf.random.uniform(shape=[50, 28, 28])\n",
    "preprocessed_images = shrink(img_batch_1)\n",
    "preprocessed_images = shrink(img_batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024815, shape=(50, 14, 14), dtype=float32, numpy=\narray([[[0.86967194, 0.04509377, 0.45983696, ..., 0.98567295,\n         0.39286077, 0.63967335],\n        [0.21514261, 0.231009  , 0.8380784 , ..., 0.12492037,\n         0.32979465, 0.7644367 ],\n        [0.39345038, 0.27786016, 0.14277947, ..., 0.6294341 ,\n         0.24087596, 0.28214848],\n        ...,\n        [0.65740764, 0.9197531 , 0.01626694, ..., 0.30248868,\n         0.07380021, 0.73587334],\n        [0.04804504, 0.7267916 , 0.12085998, ..., 0.8744234 ,\n         0.09868002, 0.46248972],\n        [0.73392856, 0.8779386 , 0.92892027, ..., 0.12233651,\n         0.07616198, 0.06904387]],\n\n       [[0.6245496 , 0.19587457, 0.01115322, ..., 0.5369823 ,\n         0.21364594, 0.56784976],\n        [0.81553376, 0.46618676, 0.8141618 , ..., 0.13088906,\n         0.0078851 , 0.32541347],\n        [0.02349722, 0.79896283, 0.87042177, ..., 0.7304561 ,\n         0.17226493, 0.9305755 ],\n        ...,\n        [0.8311995 , 0.7716377 , 0.75955737, ..., 0.61123157,\n         0.01866722, 0.0462842 ],\n        [0.01817703, 0.7118318 , 0.7593596 , ..., 0.33666015,\n         0.50184786, 0.41792178],\n        [0.27568102, 0.47436726, 0.57932985, ..., 0.18573844,\n         0.34700596, 0.8708346 ]],\n\n       [[0.03310537, 0.16590643, 0.31153548, ..., 0.888638  ,\n         0.5642463 , 0.12155032],\n        [0.24601173, 0.35853517, 0.37146866, ..., 0.01968336,\n         0.76182866, 0.854313  ],\n        [0.10184383, 0.9322293 , 0.6212357 , ..., 0.16531515,\n         0.09399939, 0.7997831 ],\n        ...,\n        [0.9243777 , 0.24196267, 0.7032105 , ..., 0.35819113,\n         0.7296883 , 0.76088905],\n        [0.22012353, 0.9952524 , 0.3836261 , ..., 0.6932355 ,\n         0.21811831, 0.05875576],\n        [0.46247303, 0.16368127, 0.6690943 , ..., 0.871974  ,\n         0.66177857, 0.8501049 ]],\n\n       ...,\n\n       [[0.23447168, 0.2085445 , 0.6517776 , ..., 0.5937332 ,\n         0.5679915 , 0.9592211 ],\n        [0.07838058, 0.5656178 , 0.10021615, ..., 0.7113761 ,\n         0.9542942 , 0.76417637],\n        [0.9141611 , 0.17603278, 0.989365  , ..., 0.985422  ,\n         0.9669943 , 0.9518181 ],\n        ...,\n        [0.7683835 , 0.8534056 , 0.70646167, ..., 0.3456018 ,\n         0.88252246, 0.8647821 ],\n        [0.45260286, 0.96517074, 0.05222881, ..., 0.3066678 ,\n         0.10177815, 0.38198614],\n        [0.17967868, 0.21540809, 0.34419417, ..., 0.7092191 ,\n         0.18716478, 0.540275  ]],\n\n       [[0.77773654, 0.45325923, 0.05749547, ..., 0.70259786,\n         0.52621794, 0.00398648],\n        [0.48884273, 0.03980136, 0.71118355, ..., 0.30768538,\n         0.12851894, 0.8404982 ],\n        [0.14555061, 0.36436856, 0.23280084, ..., 0.35423672,\n         0.47946882, 0.18117023],\n        ...,\n        [0.91808116, 0.42493308, 0.6971241 , ..., 0.265056  ,\n         0.9512335 , 0.062814  ],\n        [0.72147346, 0.6610868 , 0.7165786 , ..., 0.06889176,\n         0.15854526, 0.45508528],\n        [0.25202072, 0.05040073, 0.09531772, ..., 0.9418887 ,\n         0.8906177 , 0.6664679 ]],\n\n       [[0.80723906, 0.31032348, 0.38933456, ..., 0.5014584 ,\n         0.04435432, 0.68826926],\n        [0.79580534, 0.02999198, 0.2748475 , ..., 0.10244405,\n         0.9914273 , 0.6430557 ],\n        [0.53143895, 0.26585972, 0.93722594, ..., 0.17930543,\n         0.39761353, 0.1639905 ],\n        ...,\n        [0.11709881, 0.63966644, 0.7055992 , ..., 0.27709723,\n         0.7804669 , 0.56792355],\n        [0.8368727 , 0.719731  , 0.1980493 , ..., 0.21889281,\n         0.16248512, 0.88088584],\n        [0.6331402 , 0.26695538, 0.63183534, ..., 0.16476357,\n         0.42641294, 0.5567312 ]]], dtype=float32)>"
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "可以看出，当指定了input_signature后。 concrete function就只生成一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[0.78602743 0.10465968]\n  [0.36942482 0.7535137 ]]\n\n [[0.60974383 0.9704398 ]\n  [0.16899133 0.05010164]]], shape=(2, 2, 2), dtype=float32))\n  input_signature: (\n    TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None))\n"
    }
   ],
   "source": [
    "img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\n",
    "try:\n",
    "    p = shrink(img_batch_3)\n",
    "except ValueError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Autograph To capture Control Flow\n",
    "如果我们在函数中用到了循环，在生成的图中，可能会包含很多的op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in range(10):\n",
    "        x += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024849, shape=(), dtype=int32, numpy=15>"
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Operation 'x' type=Placeholder>,\n <tf.Operation 'add/y' type=Const>,\n <tf.Operation 'add' type=AddV2>,\n <tf.Operation 'add_1/y' type=Const>,\n <tf.Operation 'add_1' type=AddV2>,\n <tf.Operation 'add_2/y' type=Const>,\n <tf.Operation 'add_2' type=AddV2>,\n <tf.Operation 'add_3/y' type=Const>,\n <tf.Operation 'add_3' type=AddV2>,\n <tf.Operation 'add_4/y' type=Const>,\n <tf.Operation 'add_4' type=AddV2>,\n <tf.Operation 'add_5/y' type=Const>,\n <tf.Operation 'add_5' type=AddV2>,\n <tf.Operation 'add_6/y' type=Const>,\n <tf.Operation 'add_6' type=AddV2>,\n <tf.Operation 'add_7/y' type=Const>,\n <tf.Operation 'add_7' type=AddV2>,\n <tf.Operation 'add_8/y' type=Const>,\n <tf.Operation 'add_8' type=AddV2>,\n <tf.Operation 'add_9/y' type=Const>,\n <tf.Operation 'add_9' type=AddV2>,\n <tf.Operation 'Identity' type=Identity>]"
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "解决这个问题可以使用`tf.while_loop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    condition = lambda i, x: tf.less(i, 10)\n",
    "    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))\n",
    "    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])\n",
    "    return final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024893, shape=(), dtype=int32, numpy=15>"
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Operation 'x' type=Placeholder>,\n <tf.Operation 'Const' type=Const>,\n <tf.Operation 'while/maximum_iterations' type=Const>,\n <tf.Operation 'while/loop_counter' type=Const>,\n <tf.Operation 'while' type=While>,\n <tf.Operation 'while/Identity' type=Identity>,\n <tf.Operation 'while/Identity_1' type=Identity>,\n <tf.Operation 'while/Identity_2' type=Identity>,\n <tf.Operation 'while/Identity_3' type=Identity>,\n <tf.Operation 'Identity' type=Identity>]"
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "另一种方法就是修改`range()`为`tf.range()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Operation 'x' type=Placeholder>,\n <tf.Operation 'range/start' type=Const>,\n <tf.Operation 'range/limit' type=Const>,\n <tf.Operation 'range/delta' type=Const>,\n <tf.Operation 'range' type=Range>,\n <tf.Operation 'while/maximum_iterations' type=Const>,\n <tf.Operation 'while/loop_counter' type=Const>,\n <tf.Operation 'while' type=While>,\n <tf.Operation 'while/Identity' type=Identity>,\n <tf.Operation 'while/Identity_1' type=Identity>,\n <tf.Operation 'while/Identity_2' type=Identity>,\n <tf.Operation 'while/Identity_3' type=Identity>,\n <tf.Operation 'while/Identity_4' type=Identity>,\n <tf.Operation 'while/Identity_5' type=Identity>,\n <tf.Operation 'Identity' type=Identity>]"
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling Variables and Other Resources in TF Functions\n",
    "像变量 queues datasets 等等，在tf中被认为是resources，这些东西当做变量传入函数参数时，实际上传入的是变量的引用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def increment(counter, c=1):\n",
    "    return counter.assign_add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024963, shape=(), dtype=int32, numpy=2>"
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment(counter)\n",
    "increment(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "name: \"counter\"\ntype: DT_RESOURCE"
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def = increment.get_concrete_function(counter).function_def\n",
    "function_def.signature.input_arg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024979, shape=(), dtype=int32, numpy=0>"
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def increment(c=1):\n",
    "    return counter.assign_add(c)\n",
    "\n",
    "increment(counter)\n",
    "increment(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "name: \"c\"\ntype: DT_RESOURCE"
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def = increment.get_concrete_function(counter).function_def\n",
    "function_def.signature.input_arg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.counter = tf.Variable(0)\n",
    "    \n",
    "    @tf.function\n",
    "    def increment(self, c=1):\n",
    "        return self.counter.assign_add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: id=1024995, shape=(), dtype=int32, numpy=2>"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "c.increment()\n",
    "c.increment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tf_code(func, experimental_optional_features=None):\n",
    "    from IPython.display import display, Markdown\n",
    "    if hasattr(func, \"python_function\"):\n",
    "        func = func.python_function\n",
    "    code = tf.autograph.to_code(func, experimental_optional_features=experimental_optional_features)\n",
    "    display(Markdown(\"```python\\n{}\\n```\".format(code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "```python\ndef tf__add_10(x):\n  do_return = False\n  retval_ = ag__.UndefinedReturnValue()\n  with ag__.FunctionScope('add_10', 'add_10_scope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as add_10_scope:\n\n    def get_state():\n      return ()\n\n    def set_state(_):\n      pass\n\n    def loop_body(iterates, x):\n      i = iterates\n      x += 1\n      return x,\n    x, = ag__.for_stmt(ag__.converted_call(tf.range, add_10_scope.callopts, (10,), None, add_10_scope), None, loop_body, get_state, set_state, (x,), ('x',), ())\n    do_return = True\n    retval_ = add_10_scope.mark_return_value(x)\n  do_return,\n  return ag__.retval(retval_)\n\n```",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(add_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using TF Functions with tf.keras(or Not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(y_true, y_pred):\n",
    "    print(\"Tracing loss my_mse()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mae(y_true, y_pred):\n",
    "    print(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",\n",
    "            shape=(input_shape[1], self.units),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=(self.units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        print(\"Tracing MyDense.call()\")\n",
    "        return self.activation(X @ self.kernel + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = MyDense(30, activation=\"relu\")\n",
    "        self.hidden2 = MyDense(30, activation=\"relu\")\n",
    "        self.output_ = MyDense(1)\n",
    "\n",
    "    def call(self, input):\n",
    "        print(\"Tracing MyModel.call()\")\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Tracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing metric my_mae()\nTracing loss my_mse()\nTrain on 11610 samples, validate on 3870 samples\nEpoch 1/2\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n10752/11610 [==========================>...] - ETA: 0s - loss: 1.3312 - my_mae: 0.7978Tracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n11610/11610 [==============================] - 1s 127us/sample - loss: 1.2668 - my_mae: 0.7744 - val_loss: 0.5985 - val_my_mae: 0.4733\nEpoch 2/2\n11610/11610 [==============================] - 1s 58us/sample - loss: 0.4475 - my_mae: 0.4733 - val_loss: 0.7195 - val_my_mae: 0.4771\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1c5807e10>"
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train_scaled, \n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    validation_data=(X_valid_scaled, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 64 samples, validate on 64 samples\nEpoch 1/2\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n32/64 [==============>...............] - ETA: 0s - loss: 5.4319 - my_mae: 1.9745Tracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n64/64 [==============================] - 0s 2ms/sample - loss: 5.5393 - my_mae: 2.0167 - val_loss: 5.6626 - val_my_mae: 2.1397\nEpoch 2/2\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n32/64 [==============>...............] - ETA: 0s - loss: 5.9965 - my_mae: 2.1116Tracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\nTracing MyModel.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing MyDense.call()\nTracing loss my_mse()\nTracing metric my_mae()\n64/64 [==============================] - 0s 1ms/sample - loss: 5.5208 - my_mae: 2.0129 - val_loss: 5.6492 - val_my_mae: 2.1370\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1c4e6bbe0>"
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(dynamic=True)\n",
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])\n",
    "model.fit(\n",
    "    X_train_scaled[:64], \n",
    "    y_train[:64],\n",
    "    epochs=2,\n",
    "    validation_data=(X_valid_scaled[:64], y_valid[:64])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMomentumOptimizer(keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Call super().__init__() and use _set_hyper() to store hyperparameters\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate)) # handle lr=learning_rate\n",
    "        self._set_hyper(\"decay\", self._initial_decay) # \n",
    "        self._set_hyper(\"momentum\", momentum)\n",
    "    \n",
    "    def _create_slots(self, var_list):\n",
    "        \"\"\"For each model variable, create the optimizer variable associated with it.\n",
    "        TensorFlow calls these optimizer variables \"slots\".\n",
    "        For momentum optimization, we need one momentum slot per model variable.\n",
    "        \"\"\"\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"momentum\")\n",
    "\n",
    "    @tf.function\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        \"\"\"Update the slots and perform one optimization step for one model variable\n",
    "        \"\"\"\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype) # handle learning rate decay\n",
    "        momentum_var = self.get_slot(var, \"momentum\")\n",
    "        momentum_hyper = self._get_hyper(\"momentum\", var_dtype)\n",
    "        momentum_var.assign(momentum_var * momentum_hyper - (1. - momentum_hyper)* grad)\n",
    "        var.assign_add(momentum_var * lr_t)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "            \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "            \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 11610 samples\nEpoch 1/5\n11610/11610 [==============================] - 1s 70us/sample - loss: 3.6732\nEpoch 2/5\n11610/11610 [==============================] - 0s 33us/sample - loss: 1.3029\nEpoch 3/5\n11610/11610 [==============================] - 0s 35us/sample - loss: 0.8070\nEpoch 4/5\n11610/11610 [==============================] - 0s 33us/sample - loss: 0.6858\nEpoch 5/5\n11610/11610 [==============================] - 0s 33us/sample - loss: 0.6483\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x157b884e0>"
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=\"mse\", optimizer=MyMomentumOptimizer())\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. 简要概括一下TensorFlow，主要特性是什么？能够说出其他的dl库吗？\n",
    "\n",
    "TensorFlow是一个开源库，适用于大型数值计算问题，它的core与numpy很相似，但是它支持GPU优化计算，支持分布式计算，计算图形分析，基于逆向传播的autodiff优化api和其他强大的api，比如`tf.keras`, `tf.data`, `tf.image`, `tf.signal`等等。\n",
    "\n",
    "其他的比较流行的深度学习库有PyTorch, MXNet, Micrisoft Cognitive Toolkit, Theano, Caffe2, Chainer.\n",
    "\n",
    "#### 2. TensorFlow是否可以替代Numpy？ 他们的不同点是什么？\n",
    "\n",
    "他们还是有一些不同的地方，首先函数的名称不一样，比如`tf.reduce_sum()`和`np.sum()`.第二，某些函数的行为不一样，比如`tf.transpose()`会创建一份变换后的copy数据，Numpy的`.T`是在原有数据上变换。第三，Numpy的数组是可变的，TensorFlow是不可变的。\n",
    "\n",
    "#### 3. `tf.range(10)`和`tf.constant(np.arange(10))`能否或得同样的结果？\n",
    "\n",
    "这两个函数都会生成一个0到9的数组，不同的是前者数组中的值的类型为int32，后者是int64，TensorFlow默认类型为32位，Numpy默认类型为64位。\n",
    "\n",
    "#### 4. 说出除了tensors之外的其他6个数据结构？\n",
    "\n",
    "sparse tensors, tensor arrays, ragged tensors, queues, string tensors, sets.\n",
    "\n",
    "#### 5. 自定义loss可以直接写一个函数，也可以写一个`keras.losses.Loss`的子类。分别在什么场景下使用？\n",
    "\n",
    "一般情况下，应该首先考虑使用一般的python函数，当自定义的loss需要很多参数或者多种状态时，应该考虑使用子类的实现方案。重写`__init__()`和`call()`,如果需要保存参数，需要重写`get_config()`。\n",
    "\n",
    "#### 6. 自定义metric可以直接写一个函数，也可以写一个`keras.metrics.Metric`的子类。分别在什么场景下使用？\n",
    "\n",
    "这个和自定义loss很像，特殊的地方在于如何计算metric，基于epoch或者基于batch。重写`__init__()`,`update_state()`,`result()`,别忘了调用`reset_states()`.如果需要保存参数，需要重写`get_config()`。\n",
    "\n",
    "#### 7. 什么时候需要自定义layer和model？\n",
    "\n",
    "当需要自定义layer或模型结构的时候\n",
    "\n",
    "#### 8. 在自定义training loop时，有哪些注意事项？\n",
    "\n",
    "在除非必要的情况下，否则不需要自定义training loop\n",
    "\n",
    "#### 9. Keras组件是否可以包含原生python代码？ 或者必须转为TF Function？\n",
    "\n",
    "Custom Keras components should be convertible to TF Functions, which means they should stick to TF operations as much as possible and respect all the rules listed in “TF Function Rules”. If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a tf.py_function() operation (but this will reduce performance and limit your model’s portability) or set dynamic=True when creating the custom layer or model (or set run_eagerly=True when calling the model’s compile() method).\n",
    "\n",
    "#### 10. 转为TF Function的主要原则是什么？\n",
    "\n",
    "#### 11. 什么情况下需要创建dynamic的模型，如何实现？ 为何不要把你的模型全部设置为dynamic？\n",
    "\n",
    "使用dynamic模型的最大用处是调试，在该模式下，不会生成TF Function，可以调试任何的原生python代码，导入外部库。要想使用该模式，在创建模型的时候设置`dynamic=True`,或者在compile时候设置`run_eagerly=True`即可。启用dynamic模式后，就无法使用TensorFlow提供的优化功能，也无法生成graph。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}